{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDjZmCQ5skbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31fc1b95-c530-435b-ebca-e16cf0288932"
      },
      "source": [
        "!pip install -q tf-models-official==2.4.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufBVp5_EbvHJ",
        "outputId": "b844604a-a026-4ca1-ce21-abd22789943e"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from scipy import stats\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "import datetime, os\n",
        "%pylab inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib\n",
        "from IPython.display import display, Image, clear_output\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import regularizers\n",
        "import math\n",
        "import h5py\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from official import nlp\n",
        "import official.nlp.optimization\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n",
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MozLQRcetBDV"
      },
      "source": [
        "# keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTo7CP6IB2fk"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=8000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myScheduler():\n",
        "    def __init__(self, goal_lr=1e-4, end_lr=1e-5, beta=1e-4, n_epochs=100, w_epochs=10):\n",
        "        self.goal_lr = goal_lr\n",
        "        self.end_lr  = end_lr\n",
        "        self.alpha   = goal_lr/w_epochs\n",
        "        self.beta    = beta\n",
        "        self.lr      = goal_lr/w_epochs\n",
        "        self.w_epochs= w_epochs\n",
        "    def get_lr(self, epoch):\n",
        "        if(epoch<self.w_epochs):\n",
        "            self.lr += self.alpha\n",
        "        else:\n",
        "            self.lr -= (np.exp(self.beta*epoch)-self.end_lr)\n",
        "        return self.lr\n",
        "\n",
        "# s = myScheduler(1e-4,\n",
        "#                 1e-5,\n",
        "#                 1e-4,\n",
        "#                 100,\n",
        "#                 20)\n",
        "# x = np.array([i for i in range(100)])\n",
        "# y = np.array([s.get_lr(i) for i in x])\n",
        "\n",
        "# sns.lineplot(x,y)"
      ],
      "metadata": {
        "id": "gAzP5Bp9QKWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XTWRzMjj4KF"
      },
      "source": [
        "def get_angles(pos, i, d_model, c1=10_000):\n",
        "  angle_rates = 1 / np.power(c1, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgnszfl-j4Ai"
      },
      "source": [
        "def positional_encoding(position, d_model, c1=10_000):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model,\n",
        "                          c1)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReUsT7i5xwS1"
      },
      "source": [
        "def tss_positional_encoding(position, d_model, tss):\n",
        "#   angle_rads = get_angles(np.abs(np.arange(position)-tss)[:, np.newaxis],\n",
        "#                           np.arange(d_model)[np.newaxis, :],\n",
        "#                           d_model)\n",
        "    angle_rads = get_angles((np.arange(position)-tss)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1cSVBSfx8Vm"
      },
      "source": [
        "# tpe = tss_positional_encoding(666, 32, 666//2)\n",
        "# fig, ax = plt.subplots(figsize=(100,50))\n",
        "# ax.matshow(tpe[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4KGPxN8cIru"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, p_enc=\"w2v\"):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "        self.p_enc = p_enc\n",
        "\n",
        "    def call(self, inputs, masked=None, training=None):\n",
        "        if self.p_enc == \"relative\":\n",
        "            x, x_q, x_k, x_v = inputs\n",
        "            if masked is not None:\n",
        "                attn_output, att_scores = self.att(x_q, x_k, x_v, return_attention_scores=True, attention_mask=masked)\n",
        "            else:\n",
        "                attn_output, att_scores = self.att(x_q, x_k, x_v, return_attention_scores=True)\n",
        "            inputs = x\n",
        "        else:\n",
        "            if masked is not None:\n",
        "                attn_output, att_scores = self.att(inputs, inputs, inputs, return_attention_scores=True, attention_mask=masked)\n",
        "            else:\n",
        "                attn_output, att_scores = self.att(inputs, inputs, inputs, return_attention_scores=True)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output), att_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttentionBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, p_enc=\"w2v\"):\n",
        "        super(CrossAttentionBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "        self.p_enc = p_enc\n",
        "\n",
        "    def call(self, inputs, masked=None, training=None):\n",
        "        query, key_val = inputs\n",
        "        if masked is not None:\n",
        "            attn_output, att_scores = self.att(query, key_val, key_val, return_attention_scores=True, attention_mask=masked)\n",
        "        else:\n",
        "            attn_output, att_scores = self.att(query, key_val, key_val, return_attention_scores=True)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(query + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output), att_scores"
      ],
      "metadata": {
        "id": "S_lbbvI-Fvz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fft2d(x, shift_freq=False):\n",
        "    fft2d = tf.math.real(tf.signal.fft2d(tf.cast(x, tf.complex64)))\n",
        "    if shift_freq:\n",
        "        fft2d = tf.signal.fftshift(fft2d, axes=(-1, -2))\n",
        "    return fft2d"
      ],
      "metadata": {
        "id": "tCeHpCcB2WD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZVNAC1FYiRB"
      },
      "source": [
        "def fft2d(x, shift_freq=False):\n",
        "    fft2d = tf.math.real(tf.signal.fft2d(tf.cast(x, tf.complex64)))\n",
        "    if shift_freq:\n",
        "        fft2d = tf.signal.fftshift(fft2d, axes=(-1, -2))\n",
        "    return fft2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fft2dI(x, shift_freq=False):\n",
        "    fft2dI = tf.math.imag(tf.signal.fft2d(tf.cast(x, tf.complex64)))\n",
        "    if shift_freq:\n",
        "        fft2dI = tf.signal.fftshift(fft2dI, axes=(-1, -2))\n",
        "    return fft2dI"
      ],
      "metadata": {
        "id": "NxNF9RYnisya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLHuXhGGYWlq"
      },
      "source": [
        "class FNETBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, rate=0.1, compression=False, shift_freq=False, asymmetric=False):\n",
        "        super(FNETBlock, self).__init__()\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        # self.dropout1 = layers.Dropout(rate)\n",
        "        # self.dropout2 = layers.Dropout(rate)\n",
        "        self.compression = compression\n",
        "        self.shift_freq  = shift_freq\n",
        "        self.asymmetric  = asymmetric\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        fft2D = fft2d(inputs, shift_freq=self.shift_freq)\n",
        "        #fft2D = self.dropout1(fft2D, training=training)\n",
        "        out1 = self.layernorm1(inputs + fft2D)\n",
        "\n",
        "        if self.shift_freq:\n",
        "            if self.compression is not False:\n",
        "                if self.asymmetric:\n",
        "                    cut = tf.cast((out1.shape[-2] * self.compression)//2, tf.int32)\n",
        "                    out1 = out1[:, out1.shape[-2]//2:-cut, :]\n",
        "                else:\n",
        "                    cut = tf.cast((out1.shape[-2] * self.compression)//2, tf.int32)\n",
        "                    out1 = out1[:, cut:-cut, :]\n",
        "        else:\n",
        "            if self.compression is not False:\n",
        "                cut = tf.cast((out1.shape[-2] * (1-self.compression))//2, tf.int32)\n",
        "                outA = out1[:, :cut+1, :]\n",
        "                outB = out1[:, -cut-1:, :]\n",
        "                out1 = tf.concat([outA, outB], 1)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        #ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChiBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, rate=0.1, compression=False, shift_freq=False, asymmetric=False):\n",
        "        super(ChiBlock, self).__init__()\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.ffn2 = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm4 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.compression = compression\n",
        "        self.shift_freq  = shift_freq\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        fft2D  = fft2d(inputs, shift_freq=self.shift_freq)\n",
        "        fft2DI = fft2dI(inputs, shift_freq=self.shift_freq)\n",
        "\n",
        "        out1 = self.layernorm1(inputs + fft2D)\n",
        "        out2 = self.layernorm3(inputs + fft2DI)\n",
        "\n",
        "        if self.shift_freq:\n",
        "            if self.compression is not False:\n",
        "                cut = tf.cast((out1.shape[-2] * self.compression)//2, tf.int32)\n",
        "                out1 = out1[:, cut:-cut, :]\n",
        "                out2 = out2[:, cut:-cut, :]\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output2 = self.ffn2(out2)\n",
        "        return self.layernorm2(out1 + ffn_output), self.layernorm4(out2 + ffn_output2)"
      ],
      "metadata": {
        "id": "VQFrCWlvjAIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSA_KOCUTugY"
      },
      "source": [
        "class TimeSkipFNETBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, rate=0.1, compression=False, shift_freq=False, asymmetric=False):\n",
        "        super(TimeSkipFNETBlock, self).__init__()\n",
        "        self.ffn1 = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        if shift_freq:\n",
        "            self.ffn2 = tf.keras.Sequential(\n",
        "                [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "            )\n",
        "            self.ffn3 = tf.keras.Sequential(\n",
        "                [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "            )\n",
        "        self.ffn4 = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm4 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm5 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm6 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm7 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm8 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.compression = compression\n",
        "        self.shift_freq  = shift_freq\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        fft2D = fft2d(inputs, shift_freq=self.shift_freq)\n",
        "        out1 = self.layernorm1(fft2D)\n",
        "        ffn_output = self.ffn1(out1)\n",
        "        finalout = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        if self.shift_freq:\n",
        "            fft2D = fft2d(finalout, shift_freq=self.shift_freq)\n",
        "            out1 = self.layernorm3(fft2D)\n",
        "            ffn_output = self.ffn2(out1)\n",
        "            finalout = self.layernorm4(out1 + ffn_output)\n",
        "\n",
        "            fft2D = fft2d(finalout, shift_freq=self.shift_freq)\n",
        "            out1 = self.layernorm5(fft2D)\n",
        "            ffn_output = self.ffn3(out1)\n",
        "            finalout = self.layernorm6(out1 + ffn_output)\n",
        "\n",
        "        fft2D = fft2d(finalout, shift_freq=self.shift_freq)\n",
        "        out1 = self.layernorm7(fft2D+inputs)\n",
        "        ffn_output = self.ffn4(out1)\n",
        "        finalout = self.layernorm8(out1 + ffn_output)\n",
        "\n",
        "        return finalout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCMvNdRhchFb"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vnetQgIvfRc"
      },
      "source": [
        "class TokenAndPositionEmbedding2(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding2, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions, tf.expand_dims(positions, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lydV7dFtpAms"
      },
      "source": [
        "class TokenAndPositionEmbedding3(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, rate=0.1):\n",
        "        super(TokenAndPositionEmbedding3, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x) + positions\n",
        "        x = self.dropout(x, training=training)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDmNdway3R6y"
      },
      "source": [
        "class TokenAndPositionEmbedding4(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, rate=0.1):\n",
        "        super(TokenAndPositionEmbedding4, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_encoding = positional_encoding(maxlen,\n",
        "                                            embed_dim)\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        self.d_model = embed_dim\n",
        "\n",
        "    def call(self, x, training):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        x = self.token_emb(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :maxlen, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwUw1arn6vyb"
      },
      "source": [
        "class TokenEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, mask_embedding=False, w2v_init=\"uniform\", pad_to_0=False):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size,\n",
        "                                          output_dim=embed_dim,\n",
        "                                          embeddings_initializer=w2v_init,\n",
        "                                          #embeddings_constraint=tf.keras.constraints.UnitNorm(axis=0),\n",
        "                                          )\n",
        "        self.mask_embedding = mask_embedding\n",
        "        self.embed_dim      = embed_dim\n",
        "        self.pad_to_0       = pad_to_0\n",
        "    def call(self, x):\n",
        "        if self.mask_embedding is not False:\n",
        "            mask = tf.cast(x != self.mask_embedding, np.float32)\n",
        "        x = self.token_emb(x)\n",
        "        if self.mask_embedding is not False:\n",
        "            mask_embedded = tf.tile(tf.expand_dims(mask, -1), [1, 1, self.embed_dim])\n",
        "            if self.pad_to_0:\n",
        "                return x*mask_embedded, mask\n",
        "            else:\n",
        "                return x, mask\n",
        "        else:\n",
        "            return x, tf.zeros([0])\n",
        "\n",
        "# print(\"orthogonal\")\n",
        "# embedding = TokenEmbedding(10, 5, 5, mask_embedding=4)\n",
        "# x = tf.cast(np.random.randint(0, 5, size=(2, 20)), np.float32)\n",
        "# z, mask = embedding(x)\n",
        "# print(\"mask \\t\\t:\", mask)\n",
        "# print(\"input \\t\\t:\", x)\n",
        "# print(\"embedding \\t\\t:\", z)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19cPDo6rCwIA"
      },
      "source": [
        "#only new_embedding\n",
        "class PositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, embed_dim, rate):\n",
        "        super(PositionEmbedding, self).__init__()\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        maxlen = tf.shape(x)[-2]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        x += self.pos_emb(positions)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3QL17fPj581"
      },
      "source": [
        "#only new_embedding\n",
        "class PositionEncoding2(layers.Layer):\n",
        "    def __init__(self, maxlen, embed_dim, rate, tss=None, c1=10_000):\n",
        "        super(PositionEncoding2, self).__init__()\n",
        "        if tss == None:\n",
        "            self.pos_encoding = positional_encoding(maxlen, embed_dim, c1)\n",
        "        else:\n",
        "            self.pos_encoding = tss_positional_encoding(maxlen, embed_dim, tss)\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        self.d_model = embed_dim\n",
        "\n",
        "    def call(self, x, training):\n",
        "        maxlen = tf.shape(x)[-2]\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :maxlen, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnPyTC6PKSvd"
      },
      "source": [
        "#only new_embedding\n",
        "class RelativePositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, embed_dim, rate):\n",
        "        super(RelativePositionEmbedding, self).__init__()\n",
        "        self.pos_emb_q = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.pos_emb_k = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.pos_emb_v = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        maxlen = tf.shape(x)[-2]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        x_q = x + self.pos_emb_q(positions)\n",
        "        x_k = x + self.pos_emb_k(positions)\n",
        "        x_v = x + self.pos_emb_v(positions)\n",
        "        x_q = self.dropout(x_q, training=training)\n",
        "        x_k = self.dropout(x_k, training=training)\n",
        "        x_v = self.dropout(x_v, training=training)\n",
        "        return x, x_q, x_k, x_v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScalarFeatureEmbedder(layers.Layer):\n",
        "    def __init__(self, vector_dim=8, embed_dim=128):\n",
        "        super(ScalarFeatureEmbedder, self).__init__()\n",
        "        self.vector_dim = vector_dim\n",
        "        self.embed_dim  = embed_dim\n",
        "        self.vector_emb = layers.Embedding(input_dim=vector_dim, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        positions = tf.range(start=0, limit=self.vector_dim, delta=1)\n",
        "        embedded = self.vector_emb(positions)\n",
        "        zeros = tf.zeros((batch_size, self.vector_dim, self.embed_dim))\n",
        "        embedded = zeros + embedded\n",
        "        scaled = tf.expand_dims(x,-1) * embedded\n",
        "        return scaled\n",
        "\n",
        "# SFE = ScalarFeatureEmbedder(8,12)\n",
        "# kernel = np.ones((2,8))\n",
        "# kernel[1:, :]=0.1\n",
        "# input = tf.convert_to_tensor(( kernel ))\n",
        "# out = SFE(input)\n",
        "\n",
        "# print(input.shape, out.shape)"
      ],
      "metadata": {
        "id": "qBoEo46_Osok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffqf4GGyeR87"
      },
      "source": [
        "class Forward(layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super(Forward, self).__init__(name = name)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oe-sR2ANfq3"
      },
      "source": [
        "class FinalEmbedder(layers.Layer):\n",
        "    def __init__(self, kernelsMat, poolVec, embed_dim,):\n",
        "        super(FinalEmbedder, self).__init__()\n",
        "        self.embedding_network = [[layers.Conv1D(filters=embed_dim, kernel_size=k_size, strides=1, padding=\"same\",\n",
        "                                dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal') for k_size in kernelsMat[i]] for i in range(len(kernelsMat))]\n",
        "        self.poolings = [layers.AveragePooling1D(pool_size=pool_size, strides=None, padding=\"valid\") for pool_size in poolVec]\n",
        "    def call(self, x, training):\n",
        "        for embedding_level, pooler in zip(self.embedding_network, self.poolings):\n",
        "            convoluted = []\n",
        "            for conv in embedding_level:\n",
        "                convoluted.append(conv(x))\n",
        "            if len(convoluted) > 0:\n",
        "                added = layers.Add()([x, *convoluted])\n",
        "            else:\n",
        "                added = x\n",
        "            pooled = pooler(added)\n",
        "            x = pooled\n",
        "        return pooled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOsXLv_twmgU"
      },
      "source": [
        "class Add_REG(layers.Layer):\n",
        "    def __init__(self, embed_dim, rate=0.01, name=None):\n",
        "        super(Add_REG, self).__init__(name = name)\n",
        "        self.reg_emb = layers.Embedding(input_dim=1, output_dim=embed_dim)\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        REG     = tf.range(start=0, limit=1, delta=1)\n",
        "        reg_emb = self.reg_emb(REG)\n",
        "        reg_emb = self.dropout(reg_emb, training=training)\n",
        "        reg_emb = tf.tile(tf.expand_dims(reg_emb, 0), [tf.shape(x)[0], 1, 1])\n",
        "        concat  = tf.concat([reg_emb, x], 1)\n",
        "        return concat\n",
        "\n",
        "# add_reg = Add_REG(32)\n",
        "# x = np.random.rand(4,5,32)\n",
        "# x = tf.cast(np.random.randint(0, 1, size=(4,5,32)), np.float32)\n",
        "# z = add_reg(x)\n",
        "# print(z.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9e2bVrLZQx1"
      },
      "source": [
        "class prepare_AttentionMask(layers.Layer):\n",
        "    def __init__(self, add_reg, pool_size, name=None):\n",
        "        super(prepare_AttentionMask, self).__init__(name = name)\n",
        "        self.add_reg = add_reg\n",
        "        self.pool_size = pool_size\n",
        "\n",
        "    def call(self, x, training):\n",
        "        x = tf.ones(tf.shape(x)) - x\n",
        "        x = tf.expand_dims(x, -1)\n",
        "        x = layers.MaxPool1D(pool_size=self.pool_size, strides=None, padding=\"valid\")(x)\n",
        "        if self.add_reg:\n",
        "            x = tf.concat([tf.zeros((tf.shape(x)[0], 1, 1)), x], axis=1)\n",
        "        x = tf.ones(tf.shape(x)) - x\n",
        "        x = tf.matmul(x, x, transpose_b=True)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class blend_buckets(layers.Layer):\n",
        "    def __init__(self, pool_size, name=None):\n",
        "        super(blend_buckets, self).__init__(name = name)\n",
        "        self.pool_size = pool_size\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x, mask = inputs\n",
        "        mask = tf.ones(tf.shape(mask)) - mask\n",
        "        mask = tf.tile(tf.expand_dims(mask, -1), [1, 1, tf.shape(x)[-1]])\n",
        "        mask = layers.MaxPool1D(pool_size=self.pool_size, strides=None, padding=\"valid\")(mask)\n",
        "        mask = tf.ones(tf.shape(mask)) - mask\n",
        "        return x*mask\n",
        "\n",
        "# print(\"orthogonal\")\n",
        "# embedding = TokenEmbedding(10, 5, 5, mask_embedding=4)\n",
        "# x = tf.cast(np.random.randint(0, 5, size=(2, 6)), np.float32)\n",
        "# z, mask = embedding(x)\n",
        "# print(\"mask \\t\\t:\", mask)\n",
        "# print(\"input \\t\\t:\", x)\n",
        "# print(\"embedding \\t\\t:\", z)\n",
        "# avg = layers.AveragePooling1D(pool_size=2, strides=None, padding=\"valid\")(z)\n",
        "# blend = blend_buckets(2)\n",
        "# result = blend([avg, mask])\n",
        "# print(\"result: \", result)"
      ],
      "metadata": {
        "id": "-2mCPfZrwRWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class add_Channel(layers.Layer):\n",
        "    def __init__(self, name=\"add_Channel\"):\n",
        "        super(add_Channel, self).__init__(name = name)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        x = tf.expand_dims(x, -1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "k-WILD4Q4StA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbzFgsGSW9nQ"
      },
      "source": [
        "def single_attentionplot(ax, catt, j, i):\n",
        "    ax.matshow(catt)\n",
        "    ax.set_xticks([int((catt.shape[0]-1)*i/6) for i in range(7)])\n",
        "    ax.set_yticks([int((catt.shape[0]-1)*i/6) for i in range(7)])\n",
        "    #ax.grid(True, alpha=0.2)\n",
        "    if i == 0 and j==0:\n",
        "        ax.set_title(f\"Layer{j+1}\\nHead{i+1}\",pad=20)\n",
        "    elif i==0:\n",
        "        ax.set_title(f\"Layer{j+1}\",pad=20)\n",
        "    elif j==0:\n",
        "        ax.set_title(f\"Head{i+1}\",pad=20)\n",
        "\n",
        "def attention_plot(atts, epoch, show=True):\n",
        "    if len(atts)==0:\n",
        "        return 0\n",
        "    collapsed_atts = []\n",
        "    for catts in atts:\n",
        "        collapsed_atts.append(np.mean(catts, axis=0, keepdims=True))\n",
        "    n_heads = atts[0].shape[1]\n",
        "    gene=13\n",
        "    fig, axs = plt.subplots(len(atts), n_heads, figsize=(30,  int(12*(1+len(atts)/4))), squeeze=False, constrained_layout=True)\n",
        "    for j, catts in enumerate(collapsed_atts):\n",
        "        for i in range(n_heads):\n",
        "            single_attentionplot(axs[j, i], catts[0, i, :, :], j, i)\n",
        "\n",
        "    fig.suptitle(f'Mean Attention Plot - Epoch: {epoch}', y=0.98)\n",
        "    plt.tight_layout()\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.savefig(f\"/tmp/att_plots/attention_plot_{epoch}\")\n",
        "\n",
        "# atts = np.random.rand(4, 8, 4, 666, 666)\n",
        "# attention_plot(atts, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20Qg-iivAveK"
      },
      "source": [
        "def single_attentionplot_REG(ax, ax_histx, ax_histy, catt, j, i):\n",
        "    ax.matshow(catt, cmap=\"viridis\")\n",
        "    ax.set_yticks([int((catt.shape[0]-1)*i/6) for i in range(7)])\n",
        "\n",
        "    ax_histx.matshow(catt[0:1, :], cmap=\"viridis\")\n",
        "    ax_histx.set(aspect=(catt.shape[0])/13)\n",
        "    ax_histy.matshow(catt[:, 0:1], cmap=\"viridis\")\n",
        "    ax_histy.set(aspect=(13/catt.shape[0]))\n",
        "\n",
        "    ax_histx.set_xticks([int((catt.shape[0]-1)*i/6) for i in range(7)])\n",
        "    ax_histx.set_yticks([0])\n",
        "    ax_histx.set_yticklabels(['[REG]'])\n",
        "\n",
        "    ax_histy.set_xticks([])\n",
        "    ax_histy.set_xticklabels([])\n",
        "    ax_histy.set_yticks([])\n",
        "\n",
        "    if i == 0 and j==0:\n",
        "        ax_histx.set_title(f\"Layer{j+1}\\nHead{i+1}\\n\",pad=20)\n",
        "    elif i==0:\n",
        "        ax_histx.set_title(f\"Layer{j+1}\\n\",pad=20)\n",
        "    elif j==0:\n",
        "        ax_histx.set_title(f\"Head{i+1}\\n\",pad=20)\n",
        "    ax.set_xticks([])\n",
        "\n",
        "    ########################################################\n",
        "def attention_plot_REG(atts, epoch, show=True, fontsize=16.0):\n",
        "    if len(atts)==0:\n",
        "        return 0\n",
        "    plt.rcParams.update({'font.size': fontsize})\n",
        "    collapsed_atts = []\n",
        "    for catts in atts:\n",
        "        collapsed_atts.append(np.mean(catts, axis=0, keepdims=True))\n",
        "\n",
        "    n_heads = atts[0].shape[1]\n",
        "    matdim = atts[0].shape[2]-1\n",
        "    gene=13\n",
        "    left, width = 0.1, 0.65\n",
        "    bottom, height = 0.1, 0.65\n",
        "    spacing = 0.000\n",
        "    fig = plt.figure(figsize=(10,  10))\n",
        "    for j, catts in enumerate(collapsed_atts):\n",
        "        for i in range(n_heads):\n",
        "            leftslack = 0.8*i\n",
        "            bottomslack = 0.8*j\n",
        "            rect_scatter = [left + leftslack,                   bottom - bottomslack,                    width, height]\n",
        "            rect_histx   = [left + leftslack,                   bottom + height + spacing - bottomslack, width, 0.05]\n",
        "            rect_histy   = [left + width + spacing + leftslack, bottom - bottomslack,                    0.05,  height]\n",
        "            ax       = fig.add_axes(rect_scatter)\n",
        "            ax_histx = fig.add_axes(rect_histx)\n",
        "            ax_histy = fig.add_axes(rect_histy)\n",
        "            single_attentionplot_REG(ax, ax_histx, ax_histy, catts[0, i, :, :], j, i)\n",
        "\n",
        "    fig.suptitle(f'Mean Attention Plot - Epoch: {epoch}')\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.savefig(f\"/tmp/att_plots/attention_plot_REG_{epoch}\")\n",
        "\n",
        "# atts = np.random.rand(4, 8, 4, 667, 667)\n",
        "# attention_plot_REG(atts, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PISGMht0xUue"
      },
      "source": [
        "class projTransformer:\n",
        "    def __init__(   self,\n",
        "                    checkpoint_dir=\"\",\n",
        "                    model_type=\"GX-BERT\",\n",
        "                    n_epochs=300,\n",
        "                    batch_size=32,\n",
        "                    learning_rate=1e-4,\n",
        "                    momentum=0.9,\n",
        "                    maxlen=10500,\n",
        "                    embed_dim=32,\n",
        "                    num_heads=4,\n",
        "                    ff_dim=64,\n",
        "                    vocab_size=5,\n",
        "                    dense=64,\n",
        "                    lr_reduction_epoch=None,\n",
        "                    dropout_rate=0.1,\n",
        "                    t_rate = 0.1,\n",
        "                    patience=20,\n",
        "                    optimizer=\"SGD\",\n",
        "                    warmup_steps = 64*20,\n",
        "                    shuffle = True,\n",
        "                    loss = \"mse\",\n",
        "                    logdir=None,\n",
        "                    n_layers1=3,\n",
        "                    n_layers2=0,\n",
        "                    compression=False,\n",
        "                    plot_attention=False,\n",
        "                    k_size = 6,\n",
        "                    pooler = \"global\",\n",
        "                    shift_freq = False,\n",
        "                    alternate = None,\n",
        "                    timeskip = False,\n",
        "                    p_enc=\"w2v\",\n",
        "                    convDownScale=True,\n",
        "                    tss=None,\n",
        "                    pool_size=30,\n",
        "                    add_reg=False,\n",
        "                    steps_per_epoch=5,\n",
        "                    reduction_factor = 5,\n",
        "                    mask_embedding = False,\n",
        "                    w2v_init = \"uniform\",\n",
        "                    output_neurons = 1,\n",
        "                    oneHot=False,\n",
        "                    n_targets=None,\n",
        "                    cardinality=16_358,\n",
        "                    normalization = \"batch\", # \"layer\" # \"batch\"\n",
        "                    chimera = False,\n",
        "                    halflife=False,\n",
        "                    asymmetric = False,\n",
        "                    pad_to_0 = False,\n",
        "                    DNA_tags = False,\n",
        "                    w2v_embdim = None,\n",
        "                    c1_posenc = 10_000,\n",
        "                    transcr_factors = False,\n",
        "                    sure = False,\n",
        "                    ):\n",
        "\n",
        "        self.checkpoint_dir     = checkpoint_dir\n",
        "        self.model_type         = model_type\n",
        "        self.n_epochs           = n_epochs\n",
        "        self.batch_size         = batch_size\n",
        "        self.learning_rate      = learning_rate\n",
        "        self.momentum           = momentum\n",
        "        self.maxlen             = maxlen\n",
        "        self.embed_dim          = embed_dim\n",
        "        self.num_heads          = num_heads\n",
        "        self.ff_dim             = ff_dim\n",
        "        self.vocab_size         = vocab_size\n",
        "        self.dense              = dense\n",
        "        self.dropout_rate       = dropout_rate\n",
        "        self.lr_reduction_epoch = lr_reduction_epoch\n",
        "        self.t_rate             = t_rate\n",
        "        self.patience           = patience\n",
        "        self.optimizer          = optimizer\n",
        "        self.warmup_steps       = warmup_steps\n",
        "        self.shuffle            = shuffle\n",
        "        self.logdir             = logdir\n",
        "        self.loss               = loss\n",
        "        self.history            = \"\"\n",
        "        self.n_layers1          = n_layers1\n",
        "        self.n_layers2          = n_layers2\n",
        "        self.compression        = compression\n",
        "        self.plot_attention     = plot_attention\n",
        "        self.k_size             = k_size\n",
        "        self.p_enc              = p_enc\n",
        "        self.pooler             = pooler\n",
        "        self.alternate          = alternate\n",
        "        self.timeskip           = timeskip\n",
        "        self.shift_freq         = shift_freq\n",
        "        self.convDownScale      = convDownScale\n",
        "        self.tss                = tss\n",
        "        self.pool_size          = pool_size\n",
        "        self.steps_per_epoch    = steps_per_epoch\n",
        "        self.add_reg            = add_reg\n",
        "        self.reduction_factor   = reduction_factor\n",
        "        self.mask_embedding     = mask_embedding\n",
        "        self.w2v_init           = w2v_init\n",
        "        self.output_neurons     = output_neurons\n",
        "        self.oneHot             = oneHot\n",
        "        self.n_targets          = n_targets\n",
        "        self.cardinality        = cardinality\n",
        "        self.normalization      = normalization\n",
        "        self.chimera            = chimera\n",
        "        self.halflife           = halflife\n",
        "        self.asymmetric         = asymmetric\n",
        "        self.pad_to_0           = pad_to_0\n",
        "        self.DNA_tags           = DNA_tags\n",
        "        self.c1_posenc          = c1_posenc\n",
        "        self.transcr_factors    = transcr_factors\n",
        "        self.sure               = sure\n",
        "\n",
        "        if w2v_embdim == None:\n",
        "            self.w2v_embdim = embed_dim\n",
        "        else:\n",
        "            self.w2v_embdim = w2v_embdim\n",
        "        if self.chimera==True:\n",
        "            self.n_layers1 = 1\n",
        "            self.n_layers2 = 2\n",
        "        ######################\n",
        "        self._build_model()\n",
        "        ######################\n",
        "\n",
        "\n",
        "        if self.n_layers2 == 0:\n",
        "            self.plot_attention = False\n",
        "\n",
        "        #self.learning_rate = tf.Variable(self.learning_rate, trainable=False, name='learning_rate')\n",
        "        #optimizer\n",
        "        if self.optimizer == \"Adam\":\n",
        "            self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        if self.optimizer == \"SGD\":\n",
        "            self.optimizer = tf.keras.optimizers.SGD(learning_rate=self.learning_rate, momentum=self.momentum)\n",
        "        if self.optimizer == \"Adadelta\":\n",
        "            self.optimizer = tf.keras.optimizers.Adadelta(learning_rate=self.learning_rate, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n",
        "        if self.optimizer == \"Adamax\":\n",
        "            self.optimizer = tf.keras.optimizers.Adamax(learning_rate=self.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\")\n",
        "        if self.optimizer == \"Original\":\n",
        "            learning_rate = CustomSchedule(self.embed_dim, self.warmup_steps)\n",
        "            self.optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "        if self.optimizer == \"BERTAdam\":\n",
        "            # Set up epochs and steps\n",
        "            train_data_size = self.cardinality\n",
        "            steps_per_epoch = int(train_data_size / self.batch_size)\n",
        "            num_train_steps = steps_per_epoch * self.n_epochs\n",
        "            warmup_steps = int(self.n_epochs * train_data_size * 0.1 / self.batch_size)\n",
        "            # creates an optimizer with learning rate schedule\n",
        "            self.optimizer = nlp.optimization.create_optimizer(\n",
        "                self.learning_rate, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n",
        "        #self.loss =  tf.keras.losses.poisson\n",
        "        if self.loss == \"mse\":\n",
        "            self.loss =  tf.keras.losses.mean_squared_error\n",
        "        else:\n",
        "            self.loss =  tf.keras.losses.poisson\n",
        "        # self.training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
        "        # self.training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('training_accuracy', dtype=tf.float32)\n",
        "        # compile\n",
        "        #self.model.compile(optimizer=optimizer, loss=self.loss)\n",
        "        #self.model.compile(optimizer=optimizer, loss=custom_loss_function)\n",
        "\n",
        "    def _build_model(self):\n",
        "        if self.model_type == \"GX-BERT\":\n",
        "            embedding_layer = TokenEmbedding(self.maxlen, self.vocab_size, self.w2v_embdim, self.mask_embedding, self.w2v_init, self.pad_to_0)\n",
        "            pooler = layers.Dense(self.embed_dim)\n",
        "\n",
        "            if self.timeskip:\n",
        "                FBlock = TimeSkipFNETBlock\n",
        "            elif self.chimera:\n",
        "                FBlock = ChiBlock\n",
        "            else:\n",
        "                FBlock = FNETBlock\n",
        "            if self.alternate is None:\n",
        "                fnet_layers  = [FBlock(self.embed_dim, self.ff_dim, self.t_rate, self.compression, self.shift_freq, self.asymmetric) for _ in range(self.n_layers1)]\n",
        "                trans_layers = [TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate, self.p_enc) for _ in range(self.n_layers2)]\n",
        "            else:\n",
        "                alternate_layers = []\n",
        "                for _ in range(self.n_layers1):\n",
        "                    for i, num in enumerate(self.alternate):\n",
        "                        if i % 2 == 0:\n",
        "                            for i in range(num):\n",
        "                                alternate_layers.append(FBlock(self.embed_dim, self.ff_dim, self.t_rate, self.compression, self.shift_freq, self.asymmetric))\n",
        "                        else:\n",
        "                            for i in range(num):\n",
        "                                alternate_layers.append(TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate, self.p_enc))\n",
        "            #inputs\n",
        "            inputs = []\n",
        "\n",
        "\n",
        "            if self.oneHot:\n",
        "                input1 = layers.Input(shape=(self.maxlen, 4), name=\"sequence\")\n",
        "                embedded = layers.Conv1D(filters=self.embed_dim, kernel_size=1, strides=1, padding=\"same\",\n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(input1)\n",
        "                inputs.append(input1)\n",
        "            else:\n",
        "                input1 = layers.Input(shape=(self.maxlen), name=\"sequence\")\n",
        "                inputs.append(input1)\n",
        "\n",
        "            if self.halflife:\n",
        "                halflife = layers.Input(shape=(8), name=\"halflife\")\n",
        "                inputs.append(halflife)\n",
        "\n",
        "            if self.DNA_tags:\n",
        "                DNA_tags = layers.Input(shape=(512), name=\"dna_tags\")\n",
        "                inputs.append(DNA_tags)\n",
        "                dna_embedding = layers.Embedding(self.vocab_size, self.embed_dim)\n",
        "                dna_tags_embedded = dna_embedding(DNA_tags)\n",
        "                # dna_tags_embedded = layers.BatchNormalization()(dna_tags_embedded)\n",
        "\n",
        "            if self.transcr_factors:\n",
        "                transcr_factors = layers.Input(shape=(181), name=\"transcr_factors\")\n",
        "                inputs.append(transcr_factors)\n",
        "            if self.sure:\n",
        "                sure = layers.Input(shape=(1), name=\"sure\")\n",
        "                inputs.append(sure)\n",
        "\n",
        "            #embedding\n",
        "            if self.oneHot == False:\n",
        "                embedded, masked = embedding_layer(input1)\n",
        "                if self.w2v_embdim != self.embed_dim:\n",
        "                    embedded = layers.Conv1D(filters=self.embed_dim, kernel_size=1, strides=1, padding=\"same\",\n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "\n",
        "\n",
        "            #cnn layers\n",
        "            if self.convDownScale:\n",
        "                if self.k_size == \"multiconv\":\n",
        "                    x1 = layers.Conv1D(filters=self.embed_dim, kernel_size=6, strides=1, padding=\"same\",\n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "                    x2 = layers.Conv1D(filters=self.embed_dim, kernel_size=9, strides=1, padding=\"same\",\n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "                    skip = layers.Add()([x1, x2, embedded])\n",
        "                elif  self.k_size == \"multiconv_proj\":\n",
        "                    x1 = layers.Conv1D(filters=self.embed_dim, kernel_size=6, strides=1, padding=\"same\",\n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "                    x2 = layers.Conv1D(filters=self.embed_dim, kernel_size=9, strides=1, padding=\"same\",\n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "                    x12 = layers.Concatenate()([x1,x2])\n",
        "                    x12 = layers.Dense(self.embed_dim, activation=\"relu\")(x12)\n",
        "                    #x12 = layers.Dropout(self.dropout_rate)(x12)\n",
        "                    skip = layers.Add()([x12, embedded])\n",
        "                elif  self.k_size == \"multiconv_proj_light\":\n",
        "                    x1 = layers.Conv1D(filters=self.embed_dim, kernel_size=6, strides=1, padding=\"same\",\n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "                    x2 = layers.Conv1D(filters=self.embed_dim, kernel_size=9, strides=1, padding=\"same\",\n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "                    x1 = layers.Dense(self.embed_dim, activation=\"relu\")(x1)\n",
        "                    x2 = layers.Dense(self.embed_dim, activation=\"relu\")(x2)\n",
        "                    #x12 = layers.Dropout(self.dropout_rate)(x12)\n",
        "                    skip = layers.Add()([x1, x2, embedded])\n",
        "                elif  self.k_size == \"head_conv\":\n",
        "                    x1 = layers.Conv1D(filters=self.embed_dim, kernel_size=6, strides=1, padding=\"same\",\n",
        "                                    dilation_rate=1, groups=self.num_heads, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "                    x2 = layers.Conv1D(filters=self.embed_dim, kernel_size=9, strides=1, padding=\"same\",\n",
        "                                    dilation_rate=1, groups=self.num_heads, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "                    skip = layers.Add()([x1, x2, embedded])\n",
        "\n",
        "                else:\n",
        "                    x = layers.Conv1D(filters=self.embed_dim, kernel_size=self.k_size, strides=1, padding=\"same\",\n",
        "                                    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
        "                    skip = layers.Add()([x, embedded])\n",
        "\n",
        "                x = layers.AveragePooling1D(pool_size=self.pool_size, strides=None, padding=\"valid\")(skip)\n",
        "            else:\n",
        "                x = embedded\n",
        "\n",
        "            if self.DNA_tags:\n",
        "                x = layers.Add()([x, dna_tags_embedded])\n",
        "\n",
        "            if self.normalization == \"batch\":\n",
        "                x = layers.BatchNormalization()(x)\n",
        "            elif self.normalization == \"layer\":\n",
        "                x = layers.LayerNormalization()(x)\n",
        "\n",
        "            posemblen = x.shape[1]\n",
        "            if self.p_enc == \"w2v\":\n",
        "                posenc_layer = PositionEmbedding(posemblen, self.embed_dim, self.t_rate)\n",
        "            elif self.p_enc == \"relative\":\n",
        "                posenc_layer = RelativePositionEmbedding(posemblen, self.embed_dim, self.t_rate)\n",
        "            else:\n",
        "                posenc_layer = PositionEncoding2(posemblen, self.embed_dim, self.t_rate, self.tss, self.c1_posenc)\n",
        "\n",
        "            x = posenc_layer(x)\n",
        "\n",
        "            # if self.halflife:\n",
        "            #     SCF = ScalarFeatureEmbedder(8, self.embed_dim)\n",
        "            #     embedded_halflife = SCF(halflife)\n",
        "            #     x = layers.Concatenate(axis=1)([x, embedded_halflife])\n",
        "            # if self.halflife:\n",
        "            #     embedded_halflife = layers.Dense(self.embed_dim)(halflife)\n",
        "            #     embedded_halflife = tf.expand_dims(embedded_halflife, axis=1)\n",
        "            #     x = layers.Concatenate(axis=1)([x, embedded_halflife])\n",
        "            # M_atts = []\n",
        "            # if self.halflife:\n",
        "            #     embedded_halflife = layers.Dense(self.embed_dim)(halflife)\n",
        "            #     embedded_halflife = tf.expand_dims(embedded_halflife, axis=1)\n",
        "            #     cab = CrossAttentionBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate, self.p_enc)\n",
        "            #     crossed_halflife, matts = cab([embedded_halflife, x])\n",
        "            #     M_atts.append(matts)\n",
        "\n",
        "            if self.mask_embedding is not False and self.n_layers1==0:\n",
        "                masked = prepare_AttentionMask(self.add_reg, self.pool_size)(masked)\n",
        "            else:\n",
        "                if self.pad_to_0:\n",
        "                    x = blend_buckets(self.pool_size)([x, masked])\n",
        "                masked = None\n",
        "\n",
        "            if self.compression == False:\n",
        "                if self.add_reg:\n",
        "                    add_reg = Add_REG(self.embed_dim)\n",
        "                    x = add_reg(x)\n",
        "\n",
        "            att_scores = []\n",
        "            #transformers\n",
        "            if self.chimera:\n",
        "                x, x2 = fnet_layers[0](x)\n",
        "                if self.compression != False:\n",
        "                    if self.add_reg:\n",
        "                        x = Add_REG(self.embed_dim)(x)\n",
        "                        x2 = Add_REG(self.embed_dim)(x2)\n",
        "                x, atts  = trans_layers[0](x)\n",
        "                att_scores.append(atts)\n",
        "                x2, atts = trans_layers[1](x2)\n",
        "                att_scores.append(atts)\n",
        "            elif self.alternate is None:\n",
        "                for i in range(self.n_layers1):\n",
        "                    x = fnet_layers[i](x)\n",
        "                ###########################\n",
        "                if self.compression != False:\n",
        "                    if self.add_reg:\n",
        "                        add_reg = Add_REG(self.embed_dim)\n",
        "                        x = add_reg(x)\n",
        "                #############################\n",
        "                for i in range(self.n_layers2):\n",
        "                    x, atts = trans_layers[i](x, masked=masked)\n",
        "                    att_scores.append(atts)\n",
        "            else:\n",
        "                for layer in alternate_layers:\n",
        "                    if isinstance(layer, FBlock):\n",
        "                        x = layer(x)\n",
        "                    else:\n",
        "                        x, atts = layer(x, masked=masked)\n",
        "                        att_scores.append(atts)\n",
        "\n",
        "            #FC\n",
        "            if self.pooler == \"global\":\n",
        "                if self.chimera:\n",
        "                    x = layers.GlobalAveragePooling1D()(x)\n",
        "                    x2 = layers.GlobalAveragePooling1D()(x2)\n",
        "                    x = layers.Flatten()(x)\n",
        "                    x2 = layers.Flatten()(x2)\n",
        "                    x = layers.Concatenate()([x, x2])\n",
        "                else:\n",
        "                    x = layers.GlobalAveragePooling1D()(x)\n",
        "            else: # tanh\n",
        "                if self.chimera:\n",
        "                    x = pooler(x[:, 0])\n",
        "                    x2 = layers.Dense(self.embed_dim)(x2[:, 0])\n",
        "                    x = layers.Flatten()(x)\n",
        "                    x2 = layers.Flatten()(x2)\n",
        "                    x = layers.Concatenate()([x, x2])\n",
        "                else:\n",
        "                    x = pooler(x[:, 0])\n",
        "                    x = tf.keras.activations.tanh(x)\n",
        "\n",
        "\n",
        "            if self.halflife:\n",
        "                x = layers.Concatenate()([x, halflife])\n",
        "\n",
        "            M_atts = []\n",
        "            #     if self.halflife:\n",
        "            #     SCF = ScalarFeatureEmbedder(8, self.embed_dim)\n",
        "            #     embedded_halflife = SCF(halflife)\n",
        "            #     x = tf.expand_dims(x, axis=1)\n",
        "            #     x = layers.Concatenate(axis=1)([x, embedded_halflife])\n",
        "            #     n_tb = 4\n",
        "            #     tbs = [TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate, self.p_enc) for _ in range(n_tb)]\n",
        "            #     add_reg_M = Add_REG(self.embed_dim)\n",
        "            #     x = add_reg_M(x)\n",
        "            #     for tb in tbs:\n",
        "            #         x, matts = tb(x)\n",
        "            #         M_atts.append(matts)\n",
        "            #     pooler_M = layers.Dense(self.embed_dim)\n",
        "            #     x = pooler_M(x[:, 0])\n",
        "            #     x = tf.keras.activations.tanh(x)\n",
        "            # if self.halflife:\n",
        "            #     x = tf.expand_dims(x, axis=1)\n",
        "            #     x = layers.Concatenate(axis=1)([x, embedded_halflife, crossed_halflife])\n",
        "            #     x = layers.Flatten()(x)\n",
        "\n",
        "            if self.transcr_factors:\n",
        "                x = layers.Concatenate()([x, transcr_factors])\n",
        "            if self.sure:\n",
        "                x = layers.Concatenate()([x, sure])\n",
        "\n",
        "            # x = layers.Flatten()(x)\n",
        "            #x = layers.Concatenate()([x, input2])\n",
        "            #dense1\n",
        "            x = layers.Dense(self.dense, activation=\"relu\")(x)\n",
        "            x = layers.Dropout(self.dropout_rate)(x)\n",
        "            #dense2\n",
        "            x = layers.Dense(self.dense, activation=\"gelu\")(x)\n",
        "            x = layers.Dropout(self.dropout_rate)(x)\n",
        "            #output\n",
        "            if self.n_targets == None:\n",
        "                output = layers.Dense(self.output_neurons, activation=\"linear\", name=\"Regression_Output\")(x)\n",
        "            else:\n",
        "                x = add_Channel()(x)\n",
        "                output = layers.Conv1D(filters=self.n_targets, kernel_size=1, strides=1, padding=\"same\",\n",
        "                                    dilation_rate=1, groups=1, activation=\"softplus\", kernel_initializer='glorot_normal')(x)\n",
        "\n",
        "            print(\"model built\")\n",
        "\n",
        "            if len(att_scores) > 0:\n",
        "                #att_scores = Forward(name=\"att_scores\")(att_scores)\n",
        "                self.model = tf.keras.Model(\n",
        "                    inputs=inputs,\n",
        "                    outputs={'Regression_Output': output, 'Attention_Scores': att_scores\n",
        "                            #  , 'M_atts': M_atts\n",
        "                             },\n",
        "                    )\n",
        "            else:\n",
        "                self.model = tf.keras.Model(\n",
        "                    inputs=inputs,\n",
        "                    outputs={'Regression_Output': output},\n",
        "                    )\n",
        "            self.model.summary()\n",
        "            img = tf.keras.utils.plot_model(self.model, f\"{self.model_type}.png\", show_shapes=True)\n",
        "            display(img)\n",
        "\n",
        "        print(f\"\\nParameters:\\n\")\n",
        "        for k, v in vars(self).items():\n",
        "            pad = ' '.join(['' for _ in range(25-len(k))])\n",
        "            print(k, f\" :{pad}\", v)\n",
        "\n",
        "\n",
        "    def train_model(self, x_train, x_val, TPU=False, cardinality=16_000, strategy=None, show_att=True):\n",
        "\n",
        "        def create_step_function(model, optimizer):\n",
        "            @tf.function\n",
        "            def train_step(batch, head='Regression_Output', optimizer_clip_norm_global=0.2):\n",
        "                with tf.GradientTape() as tape:\n",
        "                    inputs = [batch['sequence']]\n",
        "                    if self.halflife:\n",
        "                        inputs.append(batch['halflife'])\n",
        "                    if self.DNA_tags:\n",
        "                        inputs.append(batch['dna_tags'])\n",
        "                    if self.transcr_factors:\n",
        "                        inputs.append(batch['transcr_factors'])\n",
        "                    if self.sure:\n",
        "                        inputs.append(batch['sure'])\n",
        "\n",
        "                    outputs     = model(inputs, training=True)[head]\n",
        "                    vec_loss    = self.loss(batch['target'], outputs)\n",
        "                    loss        = tf.reduce_mean(vec_loss)\n",
        "\n",
        "                gradients = tape.gradient(loss, model.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))  #   skip_gradients_aggregation=False\n",
        "\n",
        "                return vec_loss, outputs\n",
        "            return train_step\n",
        "\n",
        "        def eval_step_function(model):\n",
        "            @tf.function\n",
        "            def eval_step(batch, head='Regression_Output', optimizer_clip_norm_global=0.2):\n",
        "                inputs = [batch['sequence']]\n",
        "                if self.halflife:\n",
        "                    inputs.append(batch['halflife'])\n",
        "                if self.DNA_tags:\n",
        "                    inputs.append(batch['dna_tags'])\n",
        "                if self.transcr_factors:\n",
        "                    inputs.append(batch['transcr_factors'])\n",
        "                if self.sure:\n",
        "                    inputs.append(batch['sure'])\n",
        "\n",
        "                outputs     = model(inputs, training=False)\n",
        "                if self.plot_attention:\n",
        "                    att_scores  = outputs['Attention_Scores']\n",
        "                    # m_atts = outputs['M_atts']\n",
        "                else:\n",
        "                    att_scores = None\n",
        "                    # m_atts = None\n",
        "                vec_loss    = self.loss(batch['target'], outputs[head])\n",
        "                return vec_loss, att_scores\n",
        "            return eval_step\n",
        "\n",
        "        train_step = create_step_function(self.model, self.optimizer)\n",
        "        eval_step  = eval_step_function(self.model)\n",
        "        # Train the model\n",
        "        print(\"cardinality\", cardinality)\n",
        "        steps_per_epoch = cardinality//self.batch_size\n",
        "        train_data_it   = iter(x_train)\n",
        "\n",
        "        global_step = 0\n",
        "        lowest_val_loss = np.Inf\n",
        "        patience_counter = 0\n",
        "\n",
        "        STOP = False\n",
        "\n",
        "        #first attention plot\n",
        "        if self.plot_attention:\n",
        "            batch_val = next(iter(x_val))\n",
        "            if strategy is None:\n",
        "                _, att_sco = eval_step(batch_val)\n",
        "            else:\n",
        "                _, att_sco = strategy.run(eval_step, args=(batch_val,))\n",
        "                att_sco       = strategy.gather(att_sco, axis=None)\n",
        "                # m_atts        = strategy.gather(m_atts, axis=None)\n",
        "            if self.add_reg:\n",
        "                attention_plot_REG(att_sco, 0, show_att)\n",
        "                # attention_plot_REG(m_atts, 0, show_att)\n",
        "            else:\n",
        "                attention_plot(att_sco, 0, show_att)\n",
        "                # attention_plot_REG(m_atts, 0, show_att)\n",
        "\n",
        "        self.history = {'train_loss' : [], 'val_loss': [], 'epoch': []}\n",
        "        hasattrNumpy = hasattr(self.optimizer.learning_rate, 'numpy')\n",
        "        # training\n",
        "        for epoch_i in range(self.n_epochs):\n",
        "            if STOP:\n",
        "                break\n",
        "            print(\"\\n\", f'Epoch: {epoch_i+1}',)\n",
        "            train_loss_vec = []\n",
        "            val_loss_vec   = []\n",
        "            # lr scheduler\n",
        "            if epoch_i == self.lr_reduction_epoch:\n",
        "                if hasattrNumpy:\n",
        "                    self.learning_rate = self.learning_rate/self.reduction_factor\n",
        "                    self.optimizer.learning_rate.assign(self.learning_rate)\n",
        "                    print(\"New Learning Rate Assigned: \", self.learning_rate)\n",
        "            for i in tqdm(range(steps_per_epoch)):\n",
        "                global_step += 1\n",
        "\n",
        "                # if global_step > 1:\n",
        "                #     learning_rate_frac = tf.math.minimum(\n",
        "                #         1.0, global_step / tf.math.maximum(1.0, num_warmup_steps))\n",
        "                #     learning_rate.assign(target_learning_rate * learning_rate_frac)\n",
        "\n",
        "                batch_train = next(train_data_it)\n",
        "                if strategy is None:\n",
        "                    train_loss, _     = train_step(batch=batch_train)\n",
        "                else:\n",
        "                    train_loss, o     = strategy.run(train_step, args=(batch_train,))\n",
        "                    train_loss        = strategy.gather(train_loss, axis=0)\n",
        "                    #print(\"train_loss: \", train_loss)\n",
        "                train_loss_vec.append(np.mean(train_loss.numpy()))\n",
        "\n",
        "            for i, batch_val in enumerate(x_val):\n",
        "                if strategy is None:\n",
        "                    loss, att_sco = eval_step(batch_val)\n",
        "                else:\n",
        "                    loss, att_sco = strategy.run(eval_step, args=(batch_val,))\n",
        "                    loss          = strategy.gather(loss, axis=0)\n",
        "                    att_sco       = strategy.gather(att_sco, axis=None)\n",
        "                    # m_atts        = strategy.gather(m_atts, axis=None)\n",
        "                if i == 0:\n",
        "                    atts = att_sco\n",
        "                    # matts = m_atts\n",
        "                val_loss_vec.append(np.mean(loss.numpy()))\n",
        "\n",
        "            # End of epoch.\n",
        "            train_loss = round(np.array(train_loss_vec).mean(), 3)\n",
        "            val_loss   = round(np.array(val_loss_vec).mean(), 3)\n",
        "            if hasattrNumpy:\n",
        "                lr = self.optimizer.learning_rate.numpy()\n",
        "            else:\n",
        "                lr = self.optimizer.learning_rate(global_step)\n",
        "            print(\n",
        "                    '\\t\\ttrain_loss: %.3f'  %train_loss,\n",
        "                    '\\tval_loss: %.3f'      %val_loss,\n",
        "                    '\\tlearning_rate: %.3E' %lr\n",
        "                 )\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['epoch'].append(epoch_i)\n",
        "            # patience\n",
        "            if val_loss < lowest_val_loss:\n",
        "                lowest_val_loss = val_loss\n",
        "                lowest_val_loss_epoch = epoch_i+1\n",
        "                patience_counter = 0\n",
        "                print(\"############################################ New lowest_val_loss reached #########################\")\n",
        "                # Saving the model to a path on localhost.\n",
        "                saved_model_path = \"/tmp/tf_save\"\n",
        "                save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "                self.model.save(saved_model_path, options=save_options)\n",
        "            elif patience_counter == self.patience:\n",
        "                print(  f\"End of training phase - Patience threshold reached\",\n",
        "                        f'\\nWeights Restored from Lowest val_loss epoch: {lowest_val_loss_epoch}',\n",
        "                        f'\\nlowest_val_loss: {lowest_val_loss}')\n",
        "                STOP = True\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            # attention plot\n",
        "            if self.plot_attention and epoch_i%10==0:\n",
        "                if self.add_reg:\n",
        "                    attention_plot_REG(atts, epoch_i+1, show_att)\n",
        "                    # attention_plot_REG(matts, epoch_i+1, show_att)\n",
        "                else:\n",
        "                    attention_plot(atts, epoch_i+1, show_att)\n",
        "                    # attention_plot_REG(matts, epoch_i+1, show_att)\n",
        "\n",
        "        fig, ax = plt.subplots(1, figsize=(20,9))\n",
        "        ax.plot(self.history['epoch'], self.history['train_loss'], color=\"tab:blue\")\n",
        "        ax.plot(self.history['epoch'], self.history['val_loss'], color=\"tab:orange\")\n",
        "        ax.axhline(0.42, 0, len(self.history['epoch']), alpha=0.2)\n",
        "        ax.axhline(0.40, 0, len(self.history['epoch']), alpha=0.2)\n",
        "        ax.axhline(0.36, 0, len(self.history['epoch']), alpha=0.2)\n",
        "        ax.set_xlabel(\"epoch\")\n",
        "        ax.set_ylabel(\"loss\")\n",
        "        ax.set_title('model train vs validation loss')\n",
        "        blue_patch  = mpatches.Patch(color='tab:blue', label='train')\n",
        "        orange_patch = mpatches.Patch(color='tab:orange', label='validation')\n",
        "        patches = [blue_patch, orange_patch]\n",
        "        plt.legend(handles=patches, loc='upper right')\n",
        "        fig.show()\n",
        "\n",
        "    def evaluate_model(self, dataset, head='Regression_Output', strategy = None):\n",
        "        def test_step_function(model):\n",
        "            @tf.function\n",
        "            def test_step(batch, head='Regression_Output', optimizer_clip_norm_global=0.2):\n",
        "                inputs = [batch['sequence']]\n",
        "                if self.halflife:\n",
        "                    inputs.append(batch['halflife'])\n",
        "                if self.DNA_tags:\n",
        "                    inputs.append(batch['dna_tags'])\n",
        "                if self.transcr_factors:\n",
        "                    inputs.append(batch['transcr_factors'])\n",
        "                if self.sure:\n",
        "                    inputs.append(batch['sure'])\n",
        "                output     = model(inputs, training=False)\n",
        "                return output[head]\n",
        "            return test_step\n",
        "\n",
        "        test_step = test_step_function(self.model)\n",
        "        predictions  = []\n",
        "        y            = []\n",
        "        for batch_test in dataset:\n",
        "            if strategy is None:\n",
        "                output          = test_step(batch_test)\n",
        "                predictions  += list(output.numpy().flatten())\n",
        "                y            += list(batch_test['target'].numpy().flatten())\n",
        "            else:\n",
        "                output          = strategy.run(test_step, args=(batch_test,))\n",
        "                output          = strategy.gather(output, axis=0)\n",
        "                batch_test      = strategy.gather(batch_test['target'], axis=0)\n",
        "                predictions  += list(output.numpy().flatten())\n",
        "                y            += list(batch_test.numpy().flatten())\n",
        "\n",
        "        print('y: ', len(y))\n",
        "        print('pred: ', len(predictions))\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(predictions, y)\n",
        "        pearson, _ = stats.mstats.pearsonr(predictions, y)\n",
        "        print('Test R^2 = %.3f' % r_value**2)\n",
        "        print('Test Pearson = %.3f' % pearson)\n",
        "        spearman = stats.spearmanr(predictions, y)[0]\n",
        "        print('Test Spearman = %.3f' % spearman)\n",
        "        return predictions, y\n",
        "\n",
        "    def evaluate_best_model(self, dataset, head='Regression_Output', strategy = None):\n",
        "        def test_step_function(model):\n",
        "            @tf.function\n",
        "            def test_step(batch, head='Regression_Output', optimizer_clip_norm_global=0.2):\n",
        "                inputs = [batch['sequence']]\n",
        "                if self.halflife:\n",
        "                    inputs.append(batch['halflife'])\n",
        "                if self.DNA_tags:\n",
        "                    inputs.append(batch['dna_tags'])\n",
        "                if self.transcr_factors:\n",
        "                    inputs.append(batch['transcr_factors'])\n",
        "                if self.sure:\n",
        "                    inputs.append(batch['sure'])\n",
        "                output     = model(inputs, training=False)\n",
        "                return output[head]\n",
        "            return test_step\n",
        "        # Loading the model from a path on localhost.\n",
        "        saved_model_path = \"/tmp/tf_save\"\n",
        "        if strategy is not None:\n",
        "            with strategy.scope():\n",
        "                load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "                best_model = tf.keras.models.load_model(saved_model_path, options=load_options)\n",
        "        else:\n",
        "            load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "            best_model = tf.keras.models.load_model(saved_model_path, options=load_options)\n",
        "\n",
        "        test_step = test_step_function(best_model)\n",
        "        predictions  = []\n",
        "        y            = []\n",
        "        for batch_test in dataset:\n",
        "            if strategy is None:\n",
        "                output          = test_step(batch_test)\n",
        "                predictions  += list(output.numpy().flatten())\n",
        "                y            += list(batch_test['target'].numpy().flatten())\n",
        "            else:\n",
        "                output          = strategy.run(test_step, args=(batch_test,))\n",
        "                output          = strategy.gather(output, axis=0)\n",
        "                batch_test      = strategy.gather(batch_test['target'], axis=0)\n",
        "                predictions  += list(output.numpy().flatten())\n",
        "                y            += list(batch_test.numpy().flatten())\n",
        "\n",
        "        print('y: ', len(y))\n",
        "        print('pred: ', len(predictions))\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(predictions, y)\n",
        "        pearson, _ = stats.mstats.pearsonr(predictions, y)\n",
        "        print('Test R^2 = %.3f' % r_value**2)\n",
        "        print('Test Pearson = %.3f' % pearson)\n",
        "        spearman = stats.spearmanr(predictions, y)[0]\n",
        "        print('Test Spearman = %.3f' % spearman)\n",
        "        return predictions, y\n",
        "\n",
        "    def plot_kde(self, dataset, head='Regression_Output', strategy = None, predictions=None, y=None):\n",
        "        if predictions == None and y == None:\n",
        "            predictions, y = self.evaluate_best_model(dataset, head, strategy)\n",
        "        df = pd.DataFrame({\"predictions\":predictions, \"true\":y})\n",
        "        ax = sns.displot(data=df, kde=True)\n",
        "        plt.xlabel(\"Labels\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_train(self, references=[0.36, 0.4, 0.42]):\n",
        "        fig, ax = plt.subplots(1, figsize=(20,9))\n",
        "        ax.plot(self.history['epoch'], self.history['train_loss'], color=\"tab:blue\")\n",
        "        ax.plot(self.history['epoch'], self.history['val_loss'], color=\"tab:orange\")\n",
        "        for ref in references:\n",
        "            ax.axhline(ref, 0, len(self.history['epoch']), alpha=0.2)\n",
        "        ax.set_xlabel(\"epoch\")\n",
        "        ax.set_ylabel(\"loss\")\n",
        "        ax.set_title('model train vs validation loss')\n",
        "        blue_patch  = mpatches.Patch(color='tab:blue', label='train')\n",
        "        orange_patch = mpatches.Patch(color='tab:orange', label='validation')\n",
        "        patches = [blue_patch, orange_patch]\n",
        "        plt.legend(handles=patches, loc='upper right')\n",
        "        fig.show()\n",
        "\n",
        "\n",
        "    def plot_r2(self, dataset, head='Regression_Output', strategy = None, predictions=None, y=None, xlabel=\"Predicted expression level\",\n",
        "                ylabel=\"Median expression level\", font={'family' : 'serif', 'weight' : 'normal', 'size': 24}, rcparams={'mathtext.default': 'regular', 'axes.spines.top': False, 'axes.spines.right': False}):\n",
        "        plt.rcParams.update(rcparams)\n",
        "        matplotlib.rc('font', **font)\n",
        "\n",
        "        if predictions == None and y == None:\n",
        "            predictions, y = self.evaluate_best_model(dataset, head, strategy)\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(predictions, y)\n",
        "        predictions = np.array(predictions)\n",
        "        y = np.array(y)\n",
        "        values = np.vstack([predictions, y])\n",
        "        kernel = stats.gaussian_kde(values)(values)\n",
        "\n",
        "        exp_results = {\"predicted\": predictions, \"labels\": y}\n",
        "        df = pd.DataFrame(exp_results)\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(predictions, y)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(11, 8.5))\n",
        "        sns.scatterplot(\n",
        "            data=df,\n",
        "            x=\"predicted\",\n",
        "            y=\"labels\",\n",
        "            c=kernel,\n",
        "            cmap=\"viridis\",\n",
        "            ax=ax,\n",
        "        )\n",
        "        ax.set_xlabel(f\"{xlabel}\")\n",
        "        ax.set_ylabel(f\"{ylabel}\")\n",
        "        plt.legend(loc=\"upper left\", title=f\"R$^{2}$: {round(r_value**2,3)}, n={format(len(predictions),',d')}\", frameon=False)\n",
        "\n",
        "        value_range = [np.max([np.min(predictions), np.min(y)]), np.min([np.max(predictions), np.max(y)]) ]\n",
        "        sns.lineplot(x=value_range, y=value_range, ax=ax, c=\"red\")\n",
        "        plt.show()\n",
        "        matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
        "\n",
        "    def plot_r2_deprecated(self, dataset, head='Regression_Output', strategy = None, predictions=None, y=None, ylim=(-1.5,3), xlim=(-1.5,3)):\n",
        "        if predictions == None and y == None:\n",
        "            predictions, y = self.evaluate_best_model(dataset, head, strategy)\n",
        "\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(predictions, y)\n",
        "        predictions = np.array(predictions)\n",
        "        y = np.array(y)\n",
        "        viridis = cm.get_cmap('autumn', 12)\n",
        "        diff = y - predictions\n",
        "        diff = np.abs(diff)\n",
        "\n",
        "        ### plt size\n",
        "        plt.rcParams[\"figure.figsize\"] = (10,9)\n",
        "        ### plt fontsize\n",
        "        plt.rcParams.update({'font.size': 16})\n",
        "\n",
        "        ### set title\n",
        "        plt.title(\"Expression Scatterplot\")\n",
        "        ### plot\n",
        "        bis = np.arange(-1.5, 3, 2)\n",
        "        plt.plot(bis, bis,  f\"b\", alpha=0.3)\n",
        "        for p, yi, c in zip(predictions, y, diff):\n",
        "            plt.plot(p, yi,  f\".\", markersize=10, color=viridis((1.0-c)/1.1))\n",
        "        ### set ticks\n",
        "        plt.xticks([i for i in range(-1, 4)])\n",
        "        plt.yticks([i for i in range(-1, 4)])\n",
        "        ### set labels\n",
        "        plt.xlabel(\"Predicted expression level\")\n",
        "        plt.ylabel(\"Median expression level\")\n",
        "        ### create legend\n",
        "        plt.legend(loc=\"upper right\", title=f\"r2 = %.3f\\n n = 1000\" % r_value**2)\n",
        "        ### set ylim\n",
        "        if ylim!=None:\n",
        "            plt.ylim(ylim)\n",
        "        if xlim!=None:\n",
        "            plt.xlim(xlim)\n",
        "        ### grid\n",
        "        plt.grid(alpha=0.5)\n",
        "        ### save\n",
        "        # if self.save:\n",
        "        #     plt.savefig(f\"{self.dir}{self.filename}.png\")\n",
        "        ### show\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _split_validation_data(x, y, validation_split):\n",
        "        rand_indexes = np.random.permutation(x.shape[0])\n",
        "        x = x[rand_indexes]\n",
        "        y = y[rand_indexes]\n",
        "        x_validation = x[:int(len(x) * validation_split)]\n",
        "        y_validation = y[:int(len(x) * validation_split)]\n",
        "        x_train = x[int(len(x) * validation_split):]\n",
        "        y_train = y[int(len(x) * validation_split):]\n",
        "        return x_train, y_train, x_validation, y_validation\n",
        "\n",
        "    def lr_scheduler(self, epoch, lr):\n",
        "        if epoch == self.lr_reduction_epoch:\n",
        "            return lr * 0.2\n",
        "        else:\n",
        "            return lr"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}